---
title: 'AI Needs the Humanities'
date: 2024-04-13

# permalink: /posts/2012/08/blog-post-4/
---
zachary k stine  
started: 2024-04-13    
updated:  
[add a pdf link when finished]        
# AI Needs the Humanities
        
Earlier in the year, I gave a presentation with this title at a local bioinformatics conference. The session was on AI and ethics. My basic point was that AI makes it clear that it is in humanity's best interests to support the kind of knowledge-production and skills training which happens to be done in a large-scale, efficient manner through the so-called humanities. The discourse around the broader values of humanities programs relative to their economic or scientific value, which by their nature tend to turn such broader values into measurable qualities, has always struck me as fraught. The bad takes are endless and it's honestly just kind of hard to watch remarkable scholars have to endlessly justify their existence. Part of the problem is that one's ability to appreciate the hard-to-articulate value of such skills depends on having gained exactly those skills. It's not surprising that there are people who see something like a scam in all this; certainly not something worth expending our limited resources on. *What are we getting out of all this navel-gazing? What have scholars of Greek myths, of classical Chinese thought, of games, etc. ever done that's put food on my table?*
    
Another tedious discursive faultline I find myself wishing (and failing) to avoid is that which is centered around so-called artificial intelligence. There's a funhouse mirror effect of the just-mentioned discourse present here: The salient question is how to evaluate the value of the spectrum of tools that people often mean by AI. Caricatures of the pro-AI (which sadly are what proliferate in dense social-information networks) might say that it's easy to evalute the value of such tools: there's metrics for that! When a metric gets bigger (or perhaps smaller), that means better—it's math! Or that the consistency of what such tools distill and remix with our expectations of what natural information manipulation looks like indicates, qualitatively and beyond what we can say in numbers, what it means to be better. Even those of us with a basic capacity for critical thinking can spot what's missing, what is necessary yet out of the boundaries of the system under discussion: *Yes, but what is the context in which such ends can be said to be valuable or not? What does it mean for my quality of life when that number is able to get bigger or smaller? Why should we assume it is good for a formal system to decompose and recompose the things we have made in such a way that feels like the products of humans?*
    
On a sid note, missing this line of questioning really ought to be inexcusable for those of us who have studied computing given the role played by Gödel's incompleteness theorems in our discipline's founding myths (see Petzold's The Annotated Turing for a riveting telling of this story). The parable Gödel gives us, achingly beautiful, is that anything can be unambigously distinguished, said to be true or false, within an unambiguously described system *except for the assumptions necessary for the articulation of the system itself.* We can't use claims endogenous to the system to prove the utility of the system. Instead, we have to upend the system, articulate its boundaries, expand beyond it, in order to evaluate the system; a recursive meta-perspectival loop. We can get pretty far saying things inside the system, but it's impossible to articulate, within the system, why the system has value. To articulate the system's value necessitates a change in scope that renders the system unable to resolve  There's something epistemilogically damning about this, but also liberating, I think. It makes me appreciate chaos, novelty, and humanity, but I'm getting ahead of myself.
    
One point that I made in my presentation is that AI gives us a profound articulation of why the hard-to-articulate skills, efficiently honed in humanities programs, are essential for societal decision-making. A foundational model of agency tells us that there are two equally essential ingredients in decision-making: knowledge about how things work and knowledge about the values of things. We tend to see the effects of increasing our knowledge about how things work in a very immediate, direct way in the form of evolving technologies. It requires a bit more sensitivity to see the effects of our knowledge about our values. The kinds of technologies we tend to see are an outgrowth of those values, but it's kind of hard to see this since we don't have the contrast provided by an awareness of all the other technologies we might have alternatively pursued. This also causes a very weird feedback loop: our values feed into the way things work. Our models (broadly defined) that are essential for our survival (e.g., our knowledge about how to create sufficient food) are always trying to estimate some ground truth about how things work. Yet, our values are always feeding into the actions we take, and those actions have an effect on that ground truth about how things work. Climate change is heavily influenced by our species' actions which in turn are heavily influenced by our species' values. When the climate changes, the truth about how things work changes (e.g., with respect to growing food). Culture is not just software running on separate hardware, but is actually modifying the hardware too!
    
Let's call all activities related to the interrogation, interpretation, and clarification of values the domain of evaluation. In highly complex environments with a lot of unknowns and lot of actions to consider, it's not enough for an agent to only evaluate things. Choosing to evaluate this thing over that thing is also an expression of values. In an environment where some locus of agency finds itself in an infinitely large environment of possible states and actions, an agent's ability to act reasonably also depends on how it explores. Exploration is something different from evaluation on one level, but they are intertwined given that the way in which we explore, how we evaluate which unknowns to make known, involves evaluating our options.

## Wonder, awe, and the inarticulable
        
I recently started reading Helen De Cruz's new book, *Wonderstruck*. I can't recommend it enough [include a link to it on amazon or whatever link helen has used to promote it]. I was also sent a wonderful article by C. Thi Nguyen [the limits of data] by a colleague (thanks Sharon!), which also led me to check out a podcast episode with Nguyen and Paul Smaldino from the Santa Fe Institute [link to the podcast episode]. I'm not surprised by my appreciation for Nguyen's thinking in these pieces. I have been slowly working my way through his phenomenal book, *Games*, for a while now and, as someone who thinks a lot about agency in the context of AI, I cannot recommend his deep dive into agency enough.

I want to use De Cruz and Nguyen as stand-ins for some of the ideas they express. Specifically, De Cruz's argument that wonder and awe are epistemic in nature, that they motivate us to accommodate the unknown by attempting to articulate what is currently inarticulable. At the same time, Nguyen has a lot to say in defense of what is not transparent, not clear, not articulable, and I think that fits in quite nicely here. Along this line, he also has a lot to say about why values are often lost when we mistake convenient, consistent proxies for them. There's a tie-in here with Gödel: fully and unambiguously articlating our values dooms them to only say things within the system, making our values incapable of evaluating the ground on which the system rests. Being able to unambiguously and completely define a problem is to be quite a long ways toward its solution. This is a lesson that much of computing's brief history has given us time and again, and it finds another beautiful articulation in AI (if you can hold the full game tree in memory, the algorithm for perfect play is beauitfully simple and elegant, and true for all deterministic games). Of course, the idea isn't unique to computer science. [Insert john dewey quote about stating problems and solving them here].

## Society is an AI
[* add that quote from Peli Grietzer about economic systems being algorithms with misaligned values *]


## Humanities as societal dreaming, therapy, 
[point from Why We Sleep about the important role played by dreams]

[point from ACT stuff about the therapeutic value of letting our values guide our lives; almost an independent summary 

