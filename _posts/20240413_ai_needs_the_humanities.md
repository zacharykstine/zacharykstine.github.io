---
title: 'AI Needs the Humanities'
date: 2024-04-13

# permalink: /posts/2012/08/blog-post-4/
---
zachary k stine  
started: 2024-04-13    
updated:  

# AI Needs the Humanities
        
Earlier in the year, I gave a presentation with this title at a local bioinformatics conference. The session was on AI and ethics. My basic point was that AI makes it clear that it is in humanity's best interests to support the kind of knowledge-production and skills training which happens to be done in a large-scale, efficient manner through the so-called humanities. The discourse around the broader values of humanities programs relative to their economic or scientific value, which by their nature tend to turn such broader values into measurable qualities, has always struck me as fraught. The bad takes are endless and it's honestly just kind of hard to watch remarkable scholars have to endlessly justify their existence. Part of the problem is that one's ability to appreciate the hard-to-articulate value of such skills depends on having gained exactly those skills. It's not surprising that there are people who see something like a scam in all this; certainly not something worth expending our limited resources on. *What are we getting out of all this navel-gazing? What have scholars of stories, of classical Chinese thought, of games, etc. ever done that's put food on my table?*
    
Another tedious discursive faultline I find myself wishing (and failing) to avoid is that which is centered around so-called artificial intelligence. There's a funhouse mirror effect of the just-mentioned discourse present here: The salient question is how to evaluate the value of the spectrum of tools that people often mean by AI. Caricatures of the pro-AI (which sadly are what proliferate in dense social-information networks) might say that it's easy to evalute the value of such tools: there's metrics for that! When a metric gets bigger (or perhaps smaller), that means better—it's math! Or that the consistency of what such tools distill and remix with our expectations of what natural information manipulation looks like indicates, qualitatively and beyond what we can say in numbers, what it means to be better. Even those of us with a basic capacity for critical thinking can spot what's missing, what is necessary yet out of the boundaries of the system under discussion: *Yes, but what is the context in which such ends can be said to be valuable or not? What does it mean for my quality of life when that number is able to get bigger or smaller? Why should we assume it is good for a formal system to decompose and recompose the things we have made in such a way that feels like the products of humans?*
    
On a sid note, missing this line of questioning really ought to be inexcusable for those of us who have studied computing given the role played by Gödel's incompleteness theorems in our discipline's founding myths (see Petzold's The Annotated Turing for a riveting telling of this story). The parable Gödel gives us, achingly beautiful, is that anything can be unambigously distinguished, said to be true or false, within an unambiguously described system *except for the assumptions necessary for the articulation of the system itself.* We can't use claims endogenous to the system to prove the utility of the system. Instead, we have to upend the system, articulate its boundaries, expand beyond it, in order to evaluate the system; a recursive meta-perspectival loop. We can get pretty far saying things inside the system, but it's impossible to articulate, within the system, why the system has value. To articulate the system's value necessitates a change in scope that  we make a change a scope that renders the system unable to resolve  There's something epistemilogically damning about this, but also liberating, I think. It makes me appreciate chaos, novelty, and humanity, but I'm getting ahead of myself.
    
One point that I made in my presentation is that AI gives us a profound articulation of why the hard-to-articulate skills, efficiently honed in humanities programs, are essential for societal decision-making. A foundational model of agency tells us that there are two equally essential ingredients in decision-making: knowledge about how things work and knowledge about the values of things. We tend to see the effects of increasing our knowledge about how things work in a very immediate, direct way in the form of evolving technologies. It requires a bit more sensitivity to see the effects of our knowledge about our values. The kinds of technologies we tend to see are an outgrowth of those values, but it's kind of hard to see this since we don't have the contrast provided by an awareness of all the other technologies we might have alternatively pursued. This also causes a very weird feedback loop: our values feed into the way things work. Our models (broadly defined) that are essential for our survival (e.g., our knowledge about how to create sufficient food) are always trying to estimate some ground truth about how things work. Yet, our values are always feeding into the actions we take, and those actions have an effect on that ground truth about how things work. Climate change is heavily influenced by our species' actions which in turn are heavily influenced by our species' values. When the climate changes, the truth about how things work changes (e.g., with respect to growing food). Culture is not just software running on separate hardware, but is actually modifying the hardware too!
    
Let's call all activities related to the interrogation, interpretation, and clarification of values the domain of evaluation. 

## Wonder, awe, and the inarticulable
<br>
[summarize the relevant arguments that I see De Cruz and Nguyen making]



You can have many headings
======

Aren't headings cool?
------

